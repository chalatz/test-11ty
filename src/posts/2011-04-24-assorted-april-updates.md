---
permalink: /blog/2011/04/24/assorted-april-updates/
title: "Assorted April updates"
date: 2011-04-24
classes: wide
show_date: true
---
<p>It's been a busy month, albeit without one particular thing that would merit its own post. So, in summary...</p>
<h3>Digital humanities white paper</h3>
<p>NITLE published a white paper I co-authored with Rebecca Davis, "<a href="http://www.nitle.org/live/files/36-divided-and-conquered">Divided and Conquered: How Multivarious Isolation Is Suppressing Digital Humanities Scholarship</a>" [pdf], which may be the first publication drawing extensively on the data collected during the Bamboo Planning Project (social isolation of scholars, siloed tools, difficulties involved in finding tools and content, etc.).</p>
<h3>Bamboo data sorting done</h3>
<p>I spent today doing the last data sorting for the <a href="https://wiki.projectbamboo.org/display/BPUB/Bamboo+Planning+Wiki+%28Archive%29">Bamboo Planning Project</a>. When I published the data and summaries on January 1st, I hadn't had time to include the Scholarly Narratives, or data from Workshops 3 and 4. Today, I finished posting the last of the Scholarly Narratives and Workshop 4, which wraps up the Bamboo Planning Project data. I'm considering trawling through the current <a href="https://wiki.projectbamboo.org/display/BTECH/Technology+Wiki+-+Home">Bamboo Technology Wiki</a> for additional data at some point.</p>
<h3>Slavic linguistics wiki</h3>
<p>I gave a talk on the <a href="http://www.slavistics.org">Slavic linguistics wiki</a> at the Midwest Slavic Workshop last Friday with Monica Vickers, a first-year grad student at The Ohio State University who used the wiki last quarter in an MA prep class. Slides are available on Google Docs <a href="https://docs.google.com/present/view?id=ddd22j37_129cmv3d2f4">here</a>. One of the concerns I've heard from professors about the wiki is that it provides students with a way to get out of doing their class reading. Interestingly, Monica noted that-- while the students <em>did</em> try that-- the wiki was a great way to refresh your memory about an article you've already read, but was no substitute for doing the reading when it came to giving you the ability to actively participate in a class discussion.</p>
<h3>Birchbark letters XML</h3>
<p><a href="http://www.flickr.com/photos/quinnanya/5645619201/" title="Giant cuddly oversized birchbark letters by quinn.anya, on Flickr"><img src="http://farm6.static.flickr.com/5226/5645619201_a5862dec43_m.jpg" width="240" height="160" alt="Giant cuddly oversized birchbark letters" class="alignright" /></a>For a few months, <a href="http://clover.slavic.pitt.edu/~djb/">David Birnbaum</a> and I have been working on a way to batch convert the birchbark letter transcriptions <a href="http://gramoty.ru">available online</a> into Unicode. We've finally gotten the XML file with the PUA/Unicode correspondences right, and he's mostly done with a clever bit of XSLT to actually do the conversion.</p>
<p>Relatedly, in sewing news, I turned some <a href="http://www.spoonflower.com/fabric_items/new?design_id=503567">Spoonflower fabric scraps</a> into <a href="http://www.flickr.com/photos/quinnanya/5645619201">giant cuddly oversized birchbark letters</a>.</p>
<h3>Drupal</h3>
<p>In the last month, I've built a Drupal VRE-style site for a friend working on a project to analyze Facebook posts from Tunisia and the Tunisian diaspora, and I have some sketchy notes for a write-up. At work, I built a Drupal service catalog in less than two hours (if you exclude the 30+ hours spent cleaning up messy data, doing multiple imports, etc.) A write-up is about half done. I'm dabbling with a new site that uses <a href="http://drupal.org/project/feeds">Feeds</a> to pull in weekly reports from major IT projects and display them in a way that's much more accessible than what we currently provide.</p>
<p>I'm also working on building a VRE for Bulgarian dialectology that I did a proof-of-concept for in February. Actually doing a batch import of all the pre-existing data from Word files (as opposed to manually entering data, as I did for the proof-of-concept) is going to be a task-- over 7,000 word nodes, plus maybe a thousand sentence nodes, and a handful of others. Getting the data into a form where it can be imported has also been a challenge, between cleaning up inconsistencies and human error, and figuring out the XSLT to pull the right data out of the XHTML generated by <a href="http://word2cleanhtml.com/">Word2CleanHTML</a>.</p>
<h3>Cocoon running on an Ubuntu server</h3>
<p>Thanks to Gerry Siarny, there's a proof-of-concept running on Slicehost. A blog post on how to do it will be coming soon.</p>
